{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll train a classifier to find **point behaviors** in inertial sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from ipdb import set_trace\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sktime.classification.compose import TimeSeriesForestClassifier, ColumnEnsembleClassifier\n",
    "from sktime.utils.data_processing import from_3d_numpy_to_nested\n",
    "import stickleback as sb\n",
    "from typing import List, Tuple\n",
    "\n",
    "rg = np.random.Generator(np.random.PCG64(1134))\n",
    "def flatten(list_of_lists: List[List]) -> List:\n",
    "    \"\"\"\n",
    "    Flatten a list of lists\n",
    "    \n",
    "        Parameters: \n",
    "            list_of_lists: a nested list\n",
    "        \n",
    "        Return:\n",
    "            A flattened list \n",
    "    \n",
    "    \"\"\"\n",
    "    return [i for sublist in list_of_lists for i in sublist]\n",
    "\n",
    "def assert_shape(x: np.ndarray, shape: List[int]):\n",
    "    \"\"\" \n",
    "    Assert shape of ndarray. Via https://medium.com/@nearlydaniel/assertion-of-arbitrary-array-shapes-in-python-3c96f6b7ccb4\n",
    "    \n",
    "        Parameters:\n",
    "            x: ndarray for shape assertion\n",
    "            shape: shape specification\n",
    "    \"\"\"\n",
    "    assert len(x.shape) == len(shape), (x.shape, shape)\n",
    "    for _a, _b in zip(x.shape, shape):\n",
    "        if isinstance(_b, int):\n",
    "            assert _a == _b, (x.shape, shape)\n",
    "\n",
    "def assert_dtype(x: np.ndarray, dtype: np.dtype):\n",
    "    \"\"\"\n",
    "    Assert dtype of ndarray.\n",
    "    \n",
    "        Parameters:\n",
    "            x: ndarray for dtype assertion\n",
    "            dtype: dtype specification\n",
    "    \"\"\"\n",
    "    assert issubclass(x.dtype.type, dtype), (x.dtype.type, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read example longitudinal sensor data and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'events' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-be1da24bc5ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbreath_sb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_sensors_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/development/stickleback/stickleback/stickleback.py\u001b[0m in \u001b[0;36mplot_sensors_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot_sensors_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mFigure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mevent_sensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         fig = make_subplots(rows=len(self.sensors.columns), cols=1,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'events' is not defined"
     ]
    }
   ],
   "source": [
    "# Read example data\n",
    "breath_sb = sb.stickleback.Stickleback(\n",
    "    sensors=pd.read_pickle(\"../data/bw180828-49_prh10.pkl\"), \n",
    "    events=pd.DatetimeIndex(pd.read_pickle(\"../data/bw180828-49_breaths.pkl\")),\n",
    "    win_size=40\n",
    ")\n",
    "\n",
    "breath_sb.plot_sensors_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training dataset consisting of **all** events and an equal sample size of negatives (randomly selected). We'll use 40 record windows (i.e., four seconds) and make sure negative sample windows don't overlap the positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 40\n",
    "event_idx = np.array([sensors.index.get_loc(e, method=\"nearest\") for e in events])\n",
    "\n",
    "def diff_from(xs: np.ndarray, ys: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return the array-wise least difference from another array\n",
    "    \n",
    "        Parameters:\n",
    "            xs: the basis array\n",
    "            ys: the target array\n",
    "            \n",
    "        Returns:\n",
    "            The minimum absolute difference for each element of xs from the closest value in ys\n",
    "    \"\"\"\n",
    "    return np.array([np.min(np.abs(x - ys)) for x in xs])\n",
    "\n",
    "# Valid indices for negatives\n",
    "nonevent_choices = np.array(range(win_size, len(sensors.index) - win_size - 1, win_size))\n",
    "diff_from_event = diff_from(nonevent_choices, event_idx)\n",
    "nonevent_choices = nonevent_choices[diff_from_event > win_size]\n",
    "\n",
    "# Randomly choose \n",
    "nonevent_idx = rg.choice(nonevent_choices, size=len(event_idx), replace=False)\n",
    "nonevent_idx.sort()\n",
    "\n",
    "print(\"+: \" + str(event_idx))\n",
    "print(\"-: \" + str(nonevent_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert positives and negatives to sktime-compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nested(data: pd.DataFrame, idx: np.ndarray, window_size: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract samples from longitudinal sensor data and reformat into nested sktime DataFrame format\n",
    "    \n",
    "        Parameters:\n",
    "            data: longitudinal sensor data\n",
    "            idx: indices of sample centers\n",
    "            window_size: number of records in each sample window\n",
    "        \n",
    "        Returns:\n",
    "            Sample windows in nested sktime DataFrame format\n",
    "    \"\"\"\n",
    "#     set_trace()\n",
    "    assert_shape(idx, [None])\n",
    "    assert_dtype(idx, np.integer)\n",
    "    assert idx.min() >= int(window_size / 2), \"idx out of bounds\"\n",
    "    assert idx.max() < len(data) - int(window_size / 2), \"idx out of bounds\"\n",
    "    \n",
    "    # Create a 3d numpy array of window data\n",
    "    data_3d = np.empty([len(idx), len(data.columns), window_size], float)\n",
    "    data_arr = data.to_numpy().transpose()\n",
    "    start_idx = idx - int(window_size / 2)\n",
    "    for i, start in enumerate(start_idx):\n",
    "        data_3d[i] = data_arr[:, start:(start + window_size)]\n",
    "\n",
    "    # Convert 3d numpy array to nested sktime DataFrame format\n",
    "    nested = from_3d_numpy_to_nested(data_3d)\n",
    "    nested.columns = data.columns\n",
    "    nested.index = data.index[idx]\n",
    "    \n",
    "    return nested\n",
    "\n",
    "nested_events = extract_nested(sensors, event_idx, win_size)\n",
    "nested_nonevents = extract_nested(sensors, nonevent_idx, win_size)\n",
    "clf_data = pd.concat([nested_events, nested_nonevents])\n",
    "clf_labels = [\"event\"] * len(nested_events) + [\"nonevent\"] * len(nested_nonevents)\n",
    "display(clf_data.head())\n",
    "print(clf_labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a ColumnEnsembleClassifier composed of TimeSeriesForest estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ColumnEnsembleClassifier(\n",
    "    estimators=[(\"TSF_\" + c, TimeSeriesForestClassifier(n_estimators=100), [i]) \n",
    "                for i, c in enumerate(clf_data.columns)]\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(clf_data, clf_labels, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use model to predict on entire longitudinal sensor data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nest_all(data: pd.DataFrame, window_size: int, nth: int = 1, exclude: np.ndarray = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert longitudinal sensor data into nested sktime DataFrame format\n",
    "    \n",
    "        Parameters:\n",
    "            data: longitudinal sensor data\n",
    "            window_size: number of records in each window\n",
    "            nth: nest every nth value (1 by default i.e., no skips)\n",
    "            exclude: indices to exclude (None by default)\n",
    "            \n",
    "        Return:\n",
    "            Windows in nested sktime DataFrame format\n",
    "    \"\"\"\n",
    "    if exclude is not None:\n",
    "        assert_shape(exclude, [None])\n",
    "        assert_dtype(exclude, np.integer)\n",
    "    \n",
    "    idx = np.array(range(int(window_size / 2), len(data) - int(window_size / 2), nth))\n",
    "    if exclude is not None:\n",
    "        idx = np.setdiff1d(idx, exclude)\n",
    "    return extract_nested(data, idx, window_size)\n",
    "\n",
    "sensors_nested = nest_all(sensors, win_size, nth=5)\n",
    "display(sensors_nested.head())\n",
    "\n",
    "proba = clf.predict_proba(sensors_nested)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert prediction probabilities to discrete events by finding *peaks*. Predicted events are considered *true positives* if they're within a tolerance of a labeled event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_events(event_proba: pd.Series, proba_thr: float, min_period: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert continuous event probabilities to discrete events\n",
    "    \n",
    "        Paramters:\n",
    "            event_proba: event probabilities (between 0 and 1)\n",
    "            min_period: minimum period between events\n",
    "        \n",
    "        Return:\n",
    "            A boolean series indicating predicted events\n",
    "    \"\"\"\n",
    "    proba_peaks = find_peaks(event_proba, height=proba_thr, distance=min_period)\n",
    "    result = pd.Series(False, name=\"predicted\", index=event_proba.index)\n",
    "    result.iloc[proba_peaks[0]] = True\n",
    "    return result\n",
    "\n",
    "def assess(predicted: pd.DatetimeIndex, actual: pd.DatetimeIndex, data: pd.DataFrame, tolerance: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Assess prediction accuracy\n",
    "    \n",
    "    The closest predicted event to each actual event (within tolerance) is considered a true positive. Predicted \n",
    "    events that are not the closest prediction to an actual event (or farther than tolerance) are considered false\n",
    "    positives. Actual events with no predicted event within the tolerance are considered false negatives.\n",
    "    \n",
    "        Parameters:\n",
    "            predicted: datetimes of predicted events\n",
    "            actual: datetimes of actual events\n",
    "            data: longitudinal sensor data\n",
    "            window_size: number of records per window\n",
    "            \n",
    "        Return:\n",
    "            A series indexed by time with \"TP\" for true positives, \"FP\" for false positives, \"FN\" for false\n",
    "            negatives, and \"TN\" for true negatives.\n",
    "    \"\"\"\n",
    "    # Find indices of actual and predicted in sensor data\n",
    "    actual_idx = np.array([data.index.get_loc(a) for a in actual])\n",
    "    predicted_idx = np.array([data.index.get_loc(p) for p in predicted])\n",
    "    \n",
    "    # Find closest predicted to each actual and their distance\n",
    "    closest = np.array([np.argmin(np.abs(predicted_idx - a)) for a in actual_idx])\n",
    "    distance = np.array([np.min(np.abs(predicted_idx - a)) for a in actual_idx])\n",
    "    \n",
    "    # Initialize result as true negative\n",
    "    result = pd.Series(\"TN\", name = \"outcome\", index = data.index)\n",
    "    \n",
    "    # Iterate through actual events. The closest predicted event within the tolerance is a true positive. If no\n",
    "    # predicted events are within the tolerance, the actual event is a false negative.\n",
    "    for i, (c, d) in enumerate(zip(closest, distance)):\n",
    "        if d <= tolerance:\n",
    "            result[predicted_idx[c]] = \"TP\" \n",
    "        else:\n",
    "            result[actual_idx[i]] = \"FN\"\n",
    "    # Iterate through predicted events. Any predicted events that aren't the closest to an actual event are false\n",
    "    # positives.\n",
    "    for i, p in enumerate(predicted_idx):\n",
    "        if i not in closest:\n",
    "            result[p] = \"FP\" \n",
    "        \n",
    "    # Sanity checks\n",
    "    assert (np.sum(result == \"TP\") + np.sum(result == \"FP\")) == len(predicted), \"TP + FP != count of predicted events\"\n",
    "    assert (np.sum(result == \"TP\") + np.sum(result == \"FN\")) == len(actual), \"TP + FN != count of actual events\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "def make_predictions(proba: np.ndarray, proba_index: pd.Index, sensors: pd.DataFrame, proba_thr: float,\n",
    "                     min_per: int, pred_tol: int) -> pd.DataFrame:\n",
    "    # Add prediction probabilities to sensor data\n",
    "    prediction_df = sensors.copy(). \\\n",
    "        join(pd.Series(proba[:, 0], name=\"event_proba\", index=proba_index)). \\\n",
    "        interpolate(method=\"cubic\"). \\\n",
    "        fillna(0)\n",
    "\n",
    "    # Add predicted events\n",
    "    prediction_df = prediction_df.join(discrete_events(prediction_df[\"event_proba\"], proba_thr, min_per))\n",
    "\n",
    "    # Add actual events\n",
    "    prediction_df = prediction_df.join(pd.DataFrame({\"actual\": True}, index=sensors.index[event_idx]))\n",
    "    prediction_df[\"actual\"].fillna(False, inplace=True) \n",
    "\n",
    "    # Check accuracy\n",
    "    prediction_df = prediction_df.join(assess(predicted=prediction_df.index[prediction_df[\"predicted\"]], \n",
    "                                              actual=prediction_df.index[prediction_df[\"actual\"]],\n",
    "                                              data=prediction_df,\n",
    "                                              tolerance=pred_tol))\n",
    "    \n",
    "    return prediction_df\n",
    "\n",
    "predictions = make_predictions(proba, sensors_nested.index, sensors, proba_thr=0.5, min_per=25, pred_tol=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot predictions for a given set of hyperparamters (probability threshold, minimum period, prediction tolerance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sensor data and predictions\n",
    "fig = make_subplots(rows=len(sensors.columns) + 1, cols=1,\n",
    "                    shared_xaxes=True,)\n",
    "predicted_only = predictions[predictions[\"predicted\"]]\n",
    "actual_only = predictions[predictions[\"actual\"]]\n",
    "\n",
    "for i, col in enumerate(list(sensors.columns) + [\"event_proba\"]):\n",
    "    # Line plot\n",
    "    fig.append_trace(go.Scatter(\n",
    "        x=predictions.index,\n",
    "        y=predictions[col],\n",
    "        mode=\"lines\"\n",
    "    ), row=i + 1, col=1)\n",
    "    # Predicted events\n",
    "    fig.append_trace(go.Scatter(\n",
    "        x=predicted_only.index,\n",
    "        y=predicted_only[col],\n",
    "        marker_color=[\"blue\" if o == \"TP\" else \"red\" for o in predicted_only[\"outcome\"]],\n",
    "        mode=\"markers\"\n",
    "    ), row=i + 1, col=1)\n",
    "    # Actual events\n",
    "    fig.append_trace(go.Scatter(\n",
    "        x=actual_only.index,\n",
    "        y=actual_only[col],\n",
    "        mode=\"markers\",\n",
    "        marker_symbol=\"circle-open\",\n",
    "        marker_size=10,\n",
    "        marker_color=\"purple\",\n",
    "    ), row=i + 1, col=1)\n",
    "    if col == \"depth\":\n",
    "        fig.update_yaxes(autorange=\"reversed\", row=i + 1, col=1)\n",
    "    fig.update_yaxes(title_text=col, row=i + 1, col=1)\n",
    "    \n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(predictions: pd.DataFrame) -> float:\n",
    "    tp = np.sum(predictions[\"outcome\"] == \"TP\")\n",
    "    fn = np.sum(predictions[\"outcome\"] == \"FN\")\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def specificity(predictions: pd.DataFrame) -> float:\n",
    "    tn = np.sum(predictions[\"outcome\"] == \"TN\")\n",
    "    fp = np.sum(predictions[\"outcome\"] == \"FP\")\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def f1(predictions: pd.DataFrame) -> float:\n",
    "    tp = np.sum(predictions[\"outcome\"] == \"TP\")\n",
    "    fp = np.sum(predictions[\"outcome\"] == \"FP\")\n",
    "    fn = np.sum(predictions[\"outcome\"] == \"FN\")\n",
    "    return 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "make_predictions2 = partial(make_predictions, proba, sensors_nested.index, sensors, min_per=25, pred_tol=50)\n",
    "thrs = np.linspace(0, 1, num=50)\n",
    "roc_tpr = np.array([sensitivity(make_predictions2(t)) for t in thrs])\n",
    "roc_fpr = 1 - np.array([specificity(make_predictions2(t)) for t in thrs])\n",
    "\n",
    "best_thr = max(thrs[roc_tpr == 1])\n",
    "\n",
    "fig = px.line(pd.DataFrame({\"TPR\": roc_tpr, \"FPR\": roc_fpr, \"thr\": thrs}), \n",
    "              x=\"FPR\", y=\"TPR\",\n",
    "              hover_data=[\"thr\"],\n",
    "              title=\"Best thr: %0.3f\" % best_thr)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add false positives to training dataset and repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refit(clf: ColumnEnsembleClassifier, sensors: pd.DataFrame, orig_X: pd.DataFrame, orig_y: List[str],\n",
    "           predictions: pd.DataFrame, window_size: int) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Re-fit a classifier by adding false positives to the training data set.\n",
    "    \n",
    "        Parameters:\n",
    "            clf: event classifier\n",
    "            sensors: longitudinal sensor data\n",
    "            orig_X: original training data\n",
    "            orig_y: original training labels\n",
    "            predictions: classifier predictions, discretized and assessed (see make_predictions())\n",
    "            window_size: size of classifier window (in records)\n",
    "            \n",
    "        Return:\n",
    "            A tuple of (new training data, new training labels)\n",
    "    \"\"\"\n",
    "    false_pos_idx = np.array([sensors.index.get_loc(i) for i in predictions.index[predictions[\"outcome\"] == \"FP\"]])\n",
    "    if len(false_pos_idx) == 0:\n",
    "        return (orig_X, orig_y)\n",
    "    new_X = pd.concat([orig_X, extract_nested(sensors, false_pos_idx, window_size)])\n",
    "    new_y = orig_y + [\"nonevent\"] * len(false_pos_idx)\n",
    "    clf.fit(new_X, new_y)\n",
    "    return (new_X, new_y)\n",
    "\n",
    "# Fit and assess\n",
    "n_refit = 5\n",
    "refits = {o: [0] * (n_refit + 1) for o in [\"TP\", \"FP\", \"FN\"]}\n",
    "new_X, new_y, new_pred = X_train, y_train, predictions\n",
    "for o in [\"TP\", \"FP\", \"FN\"]:\n",
    "    refits[o][0] = np.sum(new_pred[\"outcome\"] == o)\n",
    "for i in range(1, n_refit + 1):\n",
    "    old_y = new_y\n",
    "    new_X, new_y = refit(clf, sensors, new_X, new_y, new_pred, win_size)\n",
    "    if new_y != old_y:\n",
    "        new_proba = clf.predict_proba(sensors_nested)\n",
    "        new_pred = make_predictions(new_proba, sensors_nested.index, sensors, proba_thr=0.5, min_per=25, pred_tol=50)\n",
    "    for o in [\"TP\", \"FP\", \"FN\"]:\n",
    "        refits[o][i] = np.sum(new_pred[\"outcome\"] == o)\n",
    "refits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sensor data and predictions\n",
    "fig = make_subplots(rows=len(sensors.columns) + 1, cols=1,\n",
    "                    shared_xaxes=True)\n",
    "predicted_only = new_pred[new_pred[\"predicted\"]]\n",
    "actual_only = new_pred[new_pred[\"actual\"]]\n",
    "\n",
    "for i, col in enumerate(list(sensors.columns) + [\"event_proba\"]):\n",
    "    # Line plot\n",
    "    fig.append_trace(go.Scatter(\n",
    "        x=new_pred.index,\n",
    "        y=new_pred[col],\n",
    "        mode=\"lines\"\n",
    "    ), row=i + 1, col=1)\n",
    "    # Predicted events\n",
    "    fig.append_trace(go.Scatter(\n",
    "        x=predicted_only.index,\n",
    "        y=predicted_only[col],\n",
    "        marker_color=[\"blue\" if o == \"TP\" else \"red\" for o in predicted_only[\"outcome\"]],\n",
    "        mode=\"markers\"\n",
    "    ), row=i + 1, col=1)\n",
    "    # Actual events\n",
    "    fig.append_trace(go.Scatter(\n",
    "        x=actual_only.index,\n",
    "        y=actual_only[col],\n",
    "        mode=\"markers\",\n",
    "        marker_symbol=\"circle-open\",\n",
    "        marker_size=10,\n",
    "        marker_color=\"purple\",\n",
    "    ), row=i + 1, col=1)\n",
    "    if col == \"depth\":\n",
    "        fig.update_yaxes(autorange=\"reversed\", row=i + 1, col=1)\n",
    "    fig.update_yaxes(title_text=col, row=i + 1, col=1)\n",
    "    \n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stickleback",
   "language": "python",
   "name": "stickleback"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
